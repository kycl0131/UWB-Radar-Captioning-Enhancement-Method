Some weights of SmallCap were not initialized from the model checkpoint at Yova/SmallCap7M and are newly initialized: ['encoder_radar.layer3.2.conv3.weight', 'encoder_radar.layer3.20.conv3.weight', 'encoder_radar.layer2.2.bn2.num_batches_tracked', 'encoder_radar.layer1.2.bn1.weight', 'encoder_radar.layer3.6.bn3.running_var', 'encoder_radar.layer3.5.bn3.num_batches_tracked', 'encoder_radar.layer4.0.downsample.1.bias', 'encoder_radar.layer1.1.bn3.num_batches_tracked', 'encoder_radar.layer1.2.bn3.running_var', 'encoder_radar.layer3.3.bn2.weight', 'encoder_radar.layer3.1.bn2.weight', 'encoder_radar.layer3.12.bn3.running_var', 'encoder_radar.layer3.13.bn1.num_batches_tracked', 'encoder_radar.layer3.4.bn3.running_mean', 'encoder_radar.layer3.22.conv3.weight', 'encoder_radar.layer3.20.bn1.num_batches_tracked', 'encoder_radar.layer1.2.bn2.weight', 'encoder_radar.layer3.15.conv2.weight', 'encoder_radar.layer3.1.conv1.weight', 'encoder_radar.layer3.4.bn3.weight', 'encoder_radar.layer3.20.bn1.running_var', 'encoder_radar.layer4.1.bn3.running_var', 'encoder_radar.layer1.1.bn3.running_var', 'encoder_radar.layer4.1.bn1.bias', 'encoder_radar.layer3.11.bn3.weight', 'encoder_radar.layer3.0.bn2.bias', 'encoder_radar.layer3.5.bn2.running_var', 'encoder_radar.layer4.0.bn1.weight', 'encoder_radar.layer4.0.conv3.weight', 'encoder_radar.layer3.9.bn1.num_batches_tracked', 'encoder_radar.layer2.3.bn1.num_batches_tracked', 'encoder_radar.layer3.6.bn1.running_mean', 'encoder_radar.layer3.13.bn3.num_batches_tracked', 'encoder_radar.layer4.2.bn2.num_batches_tracked', 'encoder_radar.layer1.0.conv3.weight', 'encoder_radar.layer3.16.conv1.weight', 'encoder_radar.layer2.1.conv3.weight', 'encoder_radar.layer3.17.bn1.num_batches_tracked', 'encoder_radar.layer3.12.bn2.bias', 'encoder_radar.layer1.1.conv1.weight', 'encoder_radar.layer3.9.bn3.weight', 'encoder_radar.layer3.10.conv2.weight', 'encoder_radar.layer3.14.bn2.bias', 'encoder_radar.layer4.2.bn3.running_mean', 'encoder_radar.layer2.1.bn3.bias', 'encoder_radar.layer1.0.downsample.0.weight', 'encoder_radar.layer4.0.bn2.num_batches_tracked', 'encoder_radar.layer3.19.bn2.running_var', 'encoder_radar.layer3.8.bn1.weight', 'encoder_radar.layer2.0.bn2.weight', 'encoder_radar.layer3.7.bn3.running_var', 'encoder_radar.layer3.16.bn3.num_batches_tracked', 'encoder_radar.layer2.0.downsample.1.running_mean', 'encoder_radar.layer4.1.bn1.num_batches_tracked', 'encoder_radar.layer3.0.bn1.num_batches_tracked', 'encoder_radar.layer4.0.bn2.weight', 'encoder_radar.layer4.2.conv3.weight', 'encoder_radar.layer1.1.conv2.weight', 'encoder_radar.layer3.15.conv1.weight', 'encoder_radar.layer3.22.bn1.bias', 'encoder_radar.layer4.1.bn3.num_batches_tracked', 'encoder_radar.layer3.11.bn2.running_var', 'encoder_radar.layer3.0.bn3.bias', 'encoder_radar.layer3.17.bn1.running_mean', 'encoder_radar.layer3.21.bn1.running_var', 'encoder_radar.layer3.20.bn1.weight', 'encoder_radar.layer3.1.bn1.num_batches_tracked', 'encoder_radar.layer3.20.bn2.num_batches_tracked', 'encoder_radar.layer4.0.conv2.weight', 'encoder_radar.layer3.10.bn1.running_mean', 'encoder_radar.layer3.15.bn3.bias', 'encoder_radar.layer3.9.bn1.weight', 'encoder_radar.layer3.8.bn3.bias', 'encoder_radar.layer2.0.bn3.running_mean', 'encoder_radar.layer3.11.conv3.weight', 'encoder_radar.layer3.1.bn1.running_mean', 'encoder_radar.layer4.0.downsample.0.weight', 'encoder_radar.layer3.10.bn2.weight', 'encoder_radar.layer2.3.bn3.num_batches_tracked', 'encoder_radar.layer2.3.bn1.weight', 'encoder_radar.layer2.2.bn2.running_var', 'encoder_radar.layer3.7.bn1.running_mean', 'encoder_radar.layer3.2.bn1.running_var', 'encoder_radar.layer4.0.bn3.weight', 'encoder_radar.layer3.19.bn1.running_mean', 'encoder_radar.layer3.0.bn1.running_mean', 'encoder_radar.layer3.3.bn3.num_batches_tracked', 'encoder_radar.layer3.7.conv3.weight', 'encoder_radar.layer2.1.conv2.weight', 'encoder_radar.layer3.0.downsample.1.weight', 'encoder_radar.layer3.6.bn1.bias', 'encoder_radar.layer3.11.bn2.weight', 'encoder_radar.layer3.5.conv1.weight', 'encoder_radar.layer2.2.bn1.running_var', 'encoder_radar.layer3.1.bn
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/100:   0%|                                                                                                                                      | 0/188 [00:00<?, ?it/s]/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/functional.py:2976: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
Epoch 1/100: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [01:55<00:00,  1.63it/s, loss=0.552]
Epoch 1/100, Train Loss: 0.5521, Train_acc: 0.3317, Val Loss: 1.1014, Val_acc: 0.3320
Epoch 2/100: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [01:53<00:00,  1.66it/s, loss=0.552]
Epoch 2/100, Train Loss: 0.5521, Train_acc: 0.3325, Val Loss: 1.1013, Val_acc: 0.3322
Epoch 3/100: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [01:55<00:00,  1.63it/s, loss=0.552]
Epoch 3/100, Train Loss: 0.5521, Train_acc: 0.3320, Val Loss: 1.1013, Val_acc: 0.3322
Epoch 4/100: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [01:52<00:00,  1.67it/s, loss=0.552]
Epoch 4/100, Train Loss: 0.5521, Train_acc: 0.3322, Val Loss: 1.1014, Val_acc: 0.3317
Epoch 5/100:  45%|██████████████████████████████████████████████████▍                                                              | 84/188 [00:46<00:57,  1.82it/s, loss=0.552]
Traceback (most recent call last):
  File "/home/yunkwan/project/radarclip/distillation.py", line 403, in <module>
    save_model(classifier, '/home/yunkwan/project/radarclip/model_save/classifier')
  File "/home/yunkwan/project/radarclip/distillation.py", line 388, in main
    return
  File "/home/yunkwan/project/radarclip/distillation.py", line 210, in train_model
    param.requires_grad = False
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/tqdm/std.py", line 1195, in __iter__
    for obj in iterable:
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/yunkwan/project/radarclip/dataload.py", line 108, in __getitem__
    image = self.transform(image)
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torchvision/transforms/transforms.py", line 95, in __call__
    img = t(img)
  File "/home/yunkwan/project/radarclip/distillation.py", line 70, in __call__
    img = cv2.convertScaleAbs(img, alpha=self.alpha, beta=self.beta)
KeyboardInterrupt

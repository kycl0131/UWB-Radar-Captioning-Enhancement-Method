Traceback (most recent call last):
  File "/home/yunkwan/project/radarclip/Radarcap_main.py", line 285, in <module>
    main(args.train_1024, args.train_768, args.epochs, args.batch_size)
  File "/home/yunkwan/project/radarclip/Radarcap_main.py", line 250, in main
    retrieval_model, feature_extractor_retrieval = clip.load("RN50x64", device=device)  #1024 size 448
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/clip/clip.py", line 139, in load
    model = build_model(state_dict or model.state_dict()).to(device)
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/clip/model.py", line 424, in build_model
    model = CLIP(
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/clip/model.py", line 264, in __init__
    self.visual = ModifiedResNet(
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/clip/model.py", line 124, in __init__
    self.layer4 = self._make_layer(width * 8, layers[3], stride=2)
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/clip/model.py", line 134, in _make_layer
    layers.append(Bottleneck(self._inplanes, planes))
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/clip/model.py", line 17, in __init__
    self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 447, in __init__
    super().__init__(
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 141, in __init__
    self.reset_parameters()
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 147, in reset_parameters
    init.kaiming_uniform_(self.weight, a=math.sqrt(5))
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/init.py", line 459, in kaiming_uniform_
    return tensor.uniform_(-bound, bound, generator=generator)
KeyboardInterrupt

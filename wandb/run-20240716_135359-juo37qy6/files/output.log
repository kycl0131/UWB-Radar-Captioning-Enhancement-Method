Epoch 1/100:   0%|                                                                                                                                                                                    | 0/94 [00:01<?, ?it/s]
tensor([[[ 0.1639,  0.1785,  0.2077,  ...,  0.0033, -0.0113, -0.0259],
         [ 0.1639,  0.1785,  0.2223,  ...,  0.0179,  0.0033, -0.0113],
         [ 0.1785,  0.1931,  0.2223,  ...,  0.0471,  0.0179,  0.0033],
         ...,
         [ 0.4851,  0.4559,  0.3829,  ...,  0.5727,  0.5581,  0.5581],
         [ 0.4267,  0.3975,  0.3391,  ...,  0.5435,  0.5873,  0.6311],
         [ 0.3829,  0.3537,  0.2953,  ...,  0.5289,  0.6019,  0.6603]],

        [[ 0.2589,  0.2740,  0.3040,  ...,  0.0939,  0.0789,  0.0789],
         [ 0.2589,  0.2740,  0.3190,  ...,  0.1089,  0.0939,  0.0939],
         [ 0.2740,  0.2890,  0.3190,  ...,  0.1389,  0.1089,  0.1089],
         ...,
         [ 0.7542,  0.7242,  0.6642,  ...,  0.7992,  0.7842,  0.7992],
         [ 0.7092,  0.6792,  0.6191,  ...,  0.7242,  0.7842,  0.8142],
         [ 0.6942,  0.6642,  0.6041,  ...,  0.6792,  0.7692,  0.8142]],

        [[ 0.3684,  0.3826,  0.4110,  ...,  0.2120,  0.1693,  0.1551],
         [ 0.3684,  0.3826,  0.4253,  ...,  0.2262,  0.1835,  0.1693],
         [ 0.3826,  0.3968,  0.4253,  ...,  0.2688,  0.2262,  0.2120],
         ...,
         [ 0.8661,  0.8377,  0.7808,  ...,  0.8945,  0.8803,  0.8945],
         [ 0.8234,  0.7950,  0.7381,  ...,  0.8234,  0.8803,  0.9088],
         [ 0.8092,  0.7808,  0.7239,  ...,  0.7808,  0.8661,  0.9088]]])
Traceback (most recent call last):
  File "/home/yunkwan/project/radarclip/Radarcap_main_cl.py", line 431, in <module>
    main(args.train_3, args.epochs, args.batch_size, args.test_mode)
  File "/home/yunkwan/project/radarclip/Radarcap_main_cl.py", line 418, in main
    train_model(encoder_search_resnet1D_3, train_loader, val_loader, criterion, optimizer,warmup_scheduler,cosine_scheduler, prepro_image_fn= prepro_image_features_1024,num_epochs=num_epochs,filter = LearnableLogFilter_3)
  File "/home/yunkwan/project/radarclip/Radarcap_main_cl.py", line 216, in train_model
    images_features = prepro_image_fn(images, retrieval_model, device, batch_size=32)
  File "/home/yunkwan/project/radarclip/Radarcap_main_cl.py", line 119, in prepro_image_features_1024
    image_features_batch = retrieval_model.encode_image(batch_images) #.cpu().detach().numpy()
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/clip/model.py", line 341, in encode_image
    return self.visual(image.type(self.dtype))
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/clip/model.py", line 147, in forward
    x = stem(x)
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/clip/model.py", line 140, in stem
    x = self.relu1(self.bn1(self.conv1(x)))
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 142, in forward
    self._check_input_dim(input)
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 425, in _check_input_dim
    raise ValueError(f"expected 4D input (got {input.dim()}D input)")
ValueError: expected 4D input (got 3D input)

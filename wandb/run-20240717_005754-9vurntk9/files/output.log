Some weights of SmallCap were not initialized from the model checkpoint at Yova/SmallCap7M and are newly initialized: ['encoder_radar.layer3.20.bn1.running_var', 'encoder_radar.layer3.14.bn3.num_batches_tracked', 'encoder_radar.layer4.1.bn1.num_batches_tracked', 'encoder_radar.layer1.0.bn1.bias', 'encoder_radar.layer3.0.bn2.running_mean', 'encoder_radar.layer3.18.bn2.bias', 'encoder_radar.layer3.2.bn3.num_batches_tracked', 'encoder_radar.layer3.2.bn2.running_mean', 'encoder_radar.layer3.6.conv3.weight', 'encoder_radar.layer3.21.conv1.weight', 'encoder_radar.layer3.5.bn3.num_batches_tracked', 'encoder_radar.layer3.6.bn1.bias', 'encoder_radar.layer2.2.bn3.num_batches_tracked', 'encoder_radar.layer4.0.bn3.weight', 'encoder_radar.layer1.2.bn2.num_batches_tracked', 'encoder_radar.layer3.18.bn3.running_var', 'encoder_radar.layer3.7.bn3.running_mean', 'encoder_radar.layer3.5.bn3.running_mean', 'encoder_radar.layer3.3.bn2.num_batches_tracked', 'encoder_radar.layer2.1.bn1.weight', 'encoder_radar.layer1.0.conv2.weight', 'encoder_radar.layer3.13.bn1.running_var', 'encoder_radar.layer3.16.bn1.bias', 'encoder_radar.layer3.16.bn1.running_mean', 'encoder_radar.layer3.4.conv3.weight', 'encoder_radar.layer1.2.conv2.weight', 'encoder_radar.layer3.13.bn3.bias', 'encoder_radar.fc.bias', 'encoder_radar.layer3.15.bn1.weight', 'encoder_radar.layer3.8.bn3.running_mean', 'encoder_radar.layer3.14.conv3.weight', 'encoder_radar.layer3.1.bn2.bias', 'encoder_radar.layer3.16.bn3.bias', 'encoder_radar.layer3.3.bn2.running_mean', 'encoder_radar.layer1.2.bn3.num_batches_tracked', 'encoder_radar.layer4.1.bn1.running_mean', 'encoder_radar.layer3.12.bn1.running_mean', 'encoder_radar.layer4.0.bn1.running_mean', 'encoder_radar.layer3.19.bn3.bias', 'encoder_radar.layer4.2.bn1.weight', 'encoder_radar.layer1.0.bn2.num_batches_tracked', 'encoder_radar.layer3.11.bn2.running_mean', 'encoder_radar.layer3.13.bn2.bias', 'encoder_radar.layer2.0.bn1.running_mean', 'encoder_radar.layer4.2.bn3.running_var', 'encoder_radar.layer2.2.bn3.running_mean', 'encoder_radar.layer3.3.bn3.num_batches_tracked', 'encoder_radar.conv1.weight', 'encoder_radar.layer3.22.bn2.weight', 'encoder_radar.layer2.0.downsample.1.running_mean', 'encoder_radar.layer3.0.downsample.0.weight', 'encoder_radar.layer3.17.bn2.running_var', 'encoder_radar.layer4.1.conv3.weight', 'encoder_radar.layer1.1.conv3.weight', 'encoder_radar.layer3.0.conv2.weight', 'encoder_radar.layer3.9.bn2.bias', 'encoder_radar.layer3.3.bn3.running_mean', 'encoder_radar.layer3.10.bn2.num_batches_tracked', 'encoder_radar.layer3.14.bn3.weight', 'encoder_radar.layer2.2.conv3.weight', 'encoder_radar.layer1.1.bn1.num_batches_tracked', 'encoder_radar.layer4.0.downsample.1.running_var', 'encoder_radar.layer3.8.bn1.running_var', 'encoder_radar.layer2.0.bn3.running_mean', 'encoder_radar.layer1.0.bn3.bias', 'encoder_radar.layer2.0.bn3.running_var', 'encoder_radar.layer3.12.bn3.running_mean', 'encoder_radar.layer3.1.bn2.weight', 'encoder_radar.layer1.1.bn3.weight', 'encoder_radar.layer2.0.bn3.bias', 'encoder_radar.layer3.7.bn1.num_batches_tracked', 'encoder_radar.layer4.0.bn3.running_mean', 'encoder_radar.layer3.3.bn2.bias', 'encoder_radar.bn1.running_var', 'encoder_radar.layer3.17.bn3.running_var', 'encoder_radar.layer3.0.bn2.weight', 'encoder_radar.layer3.22.bn3.weight', 'encoder_radar.layer3.4.bn1.bias', 'encoder_radar.layer3.4.conv1.weight', 'encoder_radar.layer3.8.conv2.weight', 'encoder_radar.layer3.14.conv2.weight', 'encoder_radar.layer3.19.bn3.weight', 'encoder_radar.layer3.16.bn1.weight', 'encoder_radar.layer3.9.bn3.bias', 'encoder_radar.layer1.2.bn1.bias', 'encoder_radar.layer2.1.bn2.weight', 'encoder_radar.layer2.2.conv2.weight', 'encoder_radar.layer3.15.bn1.num_batches_tracked', 'encoder_radar.layer3.18.conv2.weight', 'encoder_radar.layer1.2.bn3.running_mean', 'encoder_radar.layer3.20.bn3.weight', 'encoder_radar.layer3.4.bn2.running_var', 'encoder_radar.layer1.1.bn1.bias', 'encoder_radar.layer2.0.downsample.0.weight', 'encoder_radar.layer3.8.bn1.running_mean', 'encoder_radar.layer3.17.bn2.bias', 'encoder_radar.layer4.0.bn3.num_batc
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/100:   0%|                                                                                                                                      | 0/188 [00:00<?, ?it/s]/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/functional.py:2976: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([128, 1536])) that is different to the input size (torch.Size([128, 768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
Epoch 1/100:   0%|                                                                                                                                      | 0/188 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/yunkwan/project/radarclip/distillation.py", line 400, in <module>
    main(args.train_3, args.epochs, args.batch_size, args.test_mode)
  File "/home/yunkwan/project/radarclip/distillation.py", line 385, in main
    train_model(teacher_model, student_model, train_loader, val_loader, criterion, optimizer, warmup_scheduler, cosine_scheduler, prepro_image_fn=prepro_image_features, num_epochs=num_epochs, filter=LearnableLogFilter_3)
  File "/home/yunkwan/project/radarclip/distillation.py", line 227, in train_model
    loss = distillation_loss((student_outputs, student_features), (teacher_outputs, teacher_features), labels)
  File "/home/yunkwan/project/radarclip/distillation.py", line 180, in distillation_loss
    feature_loss = nn.MSELoss()(student_features, teacher_features) * 0.1
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/modules/loss.py", line 535, in forward
    return F.mse_loss(input, target, reduction=self.reduction)
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/nn/functional.py", line 3365, in mse_loss
    expanded_input, expanded_target = torch.broadcast_tensors(input, target)
  File "/home/yunkwan/anaconda3/envs/smallcap/lib/python3.9/site-packages/torch/functional.py", line 76, in broadcast_tensors
    return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
RuntimeError: The size of tensor a (768) must match the size of tensor b (1536) at non-singleton dimension 1
